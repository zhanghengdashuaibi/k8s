apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-config
  namespace: flink
  labels:
    app: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager
    taskmanager.numberOfTaskSlots: 10
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 6000m
    jobmanager.memory.off-heap.size: 512m
    jobmanager.memory.jvm-metaspace.size: 1024m
    taskmanager.memory.process.size: 15360m
    taskmanager.memory.jvm-metaspace.size: 2048m
    parallelism.default: 2
  log4j-console.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
  log4j-cli.properties: |+
    ################################################################################
    #  Licensed to the Apache Software Foundation (ASF) under one
    #  or more contributor license agreements.  See the NOTICE file
    #  distributed with this work for additional information
    #  regarding copyright ownership.  The ASF licenses this file
    #  to you under the Apache License, Version 2.0 (the
    #  "License"); you may not use this file except in compliance
    #  with the License.  You may obtain a copy of the License at
    #
    #      http://www.apache.org/licenses/LICENSE-2.0
    #
    #  Unless required by applicable law or agreed to in writing, software
    #  distributed under the License is distributed on an "AS IS" BASIS,
    #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #  See the License for the specific language governing permissions and
    # limitations under the License.
    ################################################################################

    rootLogger.level = INFO
    rootLogger.appenderRef.file.ref = FileAppender

    # Log all infos in the given file
    appender.file.name = FileAppender
    appender.file.type = FILE
    appender.file.append = false
    appender.file.fileName = ${sys:log.file}
    appender.file.layout.type = PatternLayout
    appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log output from org.apache.flink.yarn to the console. This is used by the
    # CliFrontend class when using a per-job YARN cluster.
    logger.yarn.name = org.apache.flink.yarn
    logger.yarn.level = INFO
    logger.yarn.appenderRef.console.ref = ConsoleAppender
    logger.yarncli.name = org.apache.flink.yarn.cli.FlinkYarnSessionCli
    logger.yarncli.level = INFO
    logger.yarncli.appenderRef.console.ref = ConsoleAppender
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.hadoop.appenderRef.console.ref = ConsoleAppender

    # Log output from org.apache.flink.kubernetes to the console.
    logger.kubernetes.name = org.apache.flink.kubernetes
    logger.kubernetes.level = INFO
    logger.kubernetes.appenderRef.console.ref = ConsoleAppender

    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # suppress the warning that hadoop native libraries are not loaded (irrelevant for the client)
    logger.hadoopnative.name = org.apache.hadoop.util.NativeCodeLoader
    logger.hadoopnative.level = OFF

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
  log4j.properties: |+
    ################################################################################
    #  Licensed to the Apache Software Foundation (ASF) under one
    #  or more contributor license agreements.  See the NOTICE file
    #  distributed with this work for additional information
    #  regarding copyright ownership.  The ASF licenses this file
    #  to you under the Apache License, Version 2.0 (the
    #  "License"); you may not use this file except in compliance
    #  with the License.  You may obtain a copy of the License at
    #
    #      http://www.apache.org/licenses/LICENSE-2.0
    #
    #  Unless required by applicable law or agreed to in writing, software
    #  distributed under the License is distributed on an "AS IS" BASIS,
    #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #  See the License for the specific language governing permissions and
    # limitations under the License.
    ################################################################################

    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.file.ref = MainAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos in the given file
    appender.main.name = MainAppender
    appender.main.type = File
    appender.main.append = false
    appender.main.fileName = ${sys:log.file}
    appender.main.layout.type = PatternLayout
    appender.main.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
  log4j-session.properties: |+
    ################################################################################
    #  Licensed to the Apache Software Foundation (ASF) under one
    #  or more contributor license agreements.  See the NOTICE file
    #  distributed with this work for additional information
    #  regarding copyright ownership.  The ASF licenses this file
    #  to you under the Apache License, Version 2.0 (the
    #  "License"); you may not use this file except in compliance
    #  with the License.  You may obtain a copy of the License at
    #
    #      http://www.apache.org/licenses/LICENSE-2.0
    #
    #  Unless required by applicable law or agreed to in writing, software
    #  distributed under the License is distributed on an "AS IS" BASIS,
    #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #  See the License for the specific language governing permissions and
    # limitations under the License.
    ################################################################################

    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender

    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = WARN
    logger.curator.name = org.apache.flink.shaded.org.apache.curator.framework
    logger.curator.level = WARN
    logger.runtimeutils.name= org.apache.flink.runtime.util.ZooKeeperUtils
    logger.runtimeutils.level = WARN
    logger.runtimeleader.name = org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService
    logger.runtimeleader.level = WARN
  sql-client-defaults.yaml: |+
    execution:
      # select the implementation responsible for planning table programs
      # possible values are 'blink' (used by default) or 'old'
      planner: blink
      # 'batch' or 'streaming' execution
      type: streaming
      # allow 'event-time' or only 'processing-time' in sources
      time-characteristic: event-time
      # interval in ms for emitting periodic watermarks
      periodic-watermarks-interval: 200
      # 'changelog', 'table' or 'tableau' presentation of results
      result-mode: table
      # maximum number of maintained rows in 'table' presentation of results
      max-table-result-rows: 1000000
      # parallelism of the program
      parallelism: 1
      # maximum parallelism
      max-parallelism: 128
      # minimum idle state retention in ms
      min-idle-state-retention: 0
      # maximum idle state retention in ms
      max-idle-state-retention: 0
      # current catalog ('default_catalog' by default)
      current-catalog: myhive
      # current database of the current catalog (default database of the catalog by default)
      current-database: default
      # controls how table programs are restarted in case of a failures
      restart-strategy:
        # strategy type
        # possible values are "fixed-delay", "failure-rate", "none", or "fallback" (default)
        type: fallback
    catalogs:
       - name: myhive
         type: hive
         hive-conf-dir: /opt/flink/conf  # contains hive-site.xml
         hive-version: 3.1.2
    #==============================================================================
    # Configuration options
    #==============================================================================

    # Configuration options for adjusting and tuning table programs.

    # A full list of options and their default values can be found
    # on the dedicated "Configuration" web page.

    # A configuration can look like:
    # configuration:
    #   table.exec.spill-compression.enabled: true
    #   table.exec.spill-compression.block-size: 128kb
    #   table.optimizer.join-reorder-enabled: true

    #==============================================================================
    # Deployment properties
    #==============================================================================

    # Properties that describe the cluster to which table programs are submitted to.

    deployment:
      # general cluster communication timeout in ms
      response-timeout: 5000
      # (optional) address from cluster to gateway
      gateway-address: ""
      # (optional) port from cluster to gateway
      gateway-port: 0
  hive-site.xml: |+
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>csbr@hive</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://192.168.5.200:3306/metastore?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;autoReconnect=true&amp;serverTimezone=Asia/Shanghai</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
      </property>
      <property>
        <name>system:java.io.tmpdir</name>
        <value>/tmp</value>
      </property>
      <property>
        <name>system:user.name</name>
        <value>hive</value>
      </property>
      <property>
          <name>hive.metastore.schema.verification</name>
          <value>false</value>
      </property>
      <!-- 这是hiveserver2 -->
      <property>
          <name>hive.server2.thrift.port</name>
          <value>10000</value>
      </property>
      <property>
          <name>hive.server2.thrift.bind.host</name>
          <value>0.0.0.0</value>
      </property>
      <property>
        <name>hive.metastore.local</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hive-service.hive:9083</value>
      </property>
      <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
        <description>location of default database for the warehouse</description>
      </property>
      <property>
         <name>datanucleus.autoStartMechanism</name>
         <value>SchemaTable</value>
      </property>
    </configuration>